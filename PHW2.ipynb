{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b87582",
   "metadata": {},
   "source": [
    "# PHW 2 : The California Housing Prices Dataset\n",
    "> 201835503 이지민, 201835474 안해빈, 202037634 윤주은, 201835508 임윤수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f45c9",
   "metadata": {},
   "source": [
    "# Contents of Table\n",
    "> ## 1. Objective\n",
    "> ## 2. End-To-End Process\n",
    ">> ### 2-1. Data Inspection, Data Preparation\n",
    ">> ### 2-2  Analysis Algorithms, Evaluation + Choose Parameters\n",
    "> ## 3. Function - \"AutoML\"\n",
    "> ## 4. Main Program\n",
    "> ## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ae397e",
   "metadata": {},
   "source": [
    "# 1. Objective\n",
    "\n",
    "> We would like to show different combinations of algorithms to analyze a dataset\n",
    "> We will create a program structure with a \n",
    ">    single major function **“AutoML”** that will automatically run different combinations of the following : \n",
    "> -\tData Scaling, Data Encoding\n",
    "> -\tVarious Clustering Algorithms\n",
    "> -\tVarious quality measuring tools\n",
    "> -\tVarious values of parameters and hyperparameters\n",
    "> -\tVarious subsets of the features of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c6f2c",
   "metadata": {},
   "source": [
    "# 2. End-to-End Process\n",
    "# 2.1\n",
    "> ## Dataset\n",
    ">> https://www.kaggle.com/camnugent/california-housing-prices\n",
    "> ## Dataset Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353564c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data exploration\n",
    "df = pd.read_csv('housing.csv', sep=',')\n",
    "print(\"__California Housing Prices Dataset\\n\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df6191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "K = [4, 5, 6]\n",
    "\n",
    "df_sort = df.sort_values('median_house_value')\n",
    "    \n",
    "    \n",
    "for N in K :\n",
    "    \n",
    "    min = df_sort['median_house_value'].min()\n",
    "    max = df_sort['median_house_value'].max()\n",
    "    delta = max-min\n",
    "    size = delta/N\n",
    "    extra= delta*0.001\n",
    "    intervals = np.arange(min,max+extra,size)\n",
    "    #min-extra\n",
    "    intervals[0] -= delta*0.001\n",
    "    print('for N = ', N, intervals)\n",
    "    \n",
    "    n = 0\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'yellow', 'black', 'magenta']\n",
    "    count = 0\n",
    "    colors_index = 0\n",
    "    \n",
    "    for i in intervals :\n",
    "        if (df['median_house_value'][count] > n and df['median_house_value'][count] <  i) :\n",
    "            plt.scatter(df.index, df['median_house_value'], c = colors[colors_index])\n",
    "            count = count + 1\n",
    "            n = n + i\n",
    "    colors_index = colors_index + 1\n",
    "    plt.scatter(x1, y1, color = 'purple', alpha = 0.6, label = 'uniform')\n",
    "    plt.scatter(x2, y2, color = 'orange', alpha = 0.7, label = 'normal')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8adc712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def targetValueLabeling(n):\n",
    "    df = pd.read_csv('housing.csv', sep=',').iloc[:,8]\n",
    "    \n",
    "    th = (max(df) - min(df))/n\n",
    "    bins = [min(df)]\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        bins.append(th*(i+1))\n",
    "        \n",
    "    bins.append(max(df))\n",
    "    \n",
    "    bins_label = []\n",
    "    for i in range(0, n+1):\n",
    "        bins_label.append(i)\n",
    "        \n",
    "    df[\"level\"] = pd.cut(df, bins, right=True, labels=bins_label)\n",
    "    \n",
    "    return df[\"level\"]\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "targetValueLabeling(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca60fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a3523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c4842",
   "metadata": {},
   "source": [
    "> ## Data Preprocessing\n",
    "> - Drop target value : \"median_house_value\"\n",
    "> - Clean dirty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8259a17b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data preprocessing_1 : Drop attribute \"median_house_value\", because it is the answer value\n",
    "df.drop(df.columns[8], axis = 1, inplace = True)\n",
    "print(\"__Drop attribute 'median_house_value', because it is the answer value\\n\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b32c01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data preprocessing_2 : find NA\n",
    "print(\"__Find NA\\n\")\n",
    "print(df.isna().sum(), \"\\n\")\n",
    "# replace NA to MEDIAN\n",
    "df = df.replace(np.nan,int(df.iloc[:, 5].median()))\n",
    "print(\"__NA replaced to MEDIAN value. Find NA again\\n\")\n",
    "print(df.isna().sum(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb67c09e",
   "metadata": {},
   "source": [
    "# 2.2\n",
    "> ## Analysis Algorithms, Evaluation\n",
    "> ### Algo : K-means, EM(GMM), CLARANS, DBSCAN, Spectral Clustering\n",
    "> ### Scaler : StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler, Normalizer\n",
    "> ### Encoder : LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e5ceb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler, Normalizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from pyclustering.cluster.clarans import clarans\n",
    "from pyclustering.utils import timedcall\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4caee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encoding (attribute 'ocean_proximity')\n",
    "label = df['ocean_proximity']\n",
    "df_encoded = df.drop(df.columns[8], axis = 1)\n",
    "        \n",
    "#Label encoding\n",
    "le = LabelEncoder()\n",
    "le.fit(label)\n",
    "label_encoded = le.transform(label)\n",
    "df_label_encoded = pd.DataFrame(label_encoded, columns = ['labelEncoded_oceanProximity'])\n",
    "df_label_encoded = pd.concat([df_encoded, df_label_encoded], axis = 1)\n",
    "\n",
    "#Onehot encodingZ\n",
    "df_oneHot_encoded = pd.get_dummies(label)\n",
    "df_oneHot_encoded = pd.concat([df_encoded, df_oneHot_encoded], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff284483",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encoding + Scale\n",
    "df_oneHot_standard = StandardScaler().fit_transform(df_oneHot_encoded)\n",
    "df_oneHot_robust = RobustScaler().fit_transform(df_oneHot_encoded)\n",
    "df_oneHot_minmax = MinMaxScaler().fit_transform(df_oneHot_encoded)\n",
    "df_oneHot_maxabs = MaxAbsScaler().fit_transform(df_oneHot_encoded)\n",
    "df_oneHot_normalizer = Normalizer().fit_transform(df_oneHot_encoded)\n",
    "\n",
    "df_label_standard = StandardScaler().fit_transform(df_label_encoded)\n",
    "df_label_robust = RobustScaler().fit_transform(df_label_encoded)\n",
    "df_label_minmax = MinMaxScaler().fit_transform(df_label_encoded)\n",
    "df_label_maxabs = MaxAbsScaler().fit_transform(df_label_encoded)\n",
    "df_label_normalizer = Normalizer().fit_transform(df_label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb297599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change format of scaled data (np.arrays -> DataFrame)\n",
    "df_label_standard = pd.DataFrame(df_label_standard, columns=df_label_encoded.iloc[:,0:9].columns)\n",
    "df_label_robust = pd.DataFrame(df_label_robust, columns=df_label_encoded.iloc[:,0:9].columns)\n",
    "df_label_minmax = pd.DataFrame(df_label_minmax, columns=df_label_encoded.iloc[:,0:9].columns)\n",
    "df_label_maxabs = pd.DataFrame(df_label_maxabs, columns=df_label_encoded.iloc[:,0:9].columns)\n",
    "df_label_normalizer = pd.DataFrame(df_label_normalizer, columns=df_label_encoded.iloc[:,0:9].columns)\n",
    "\n",
    "df_oneHot_standard = pd.DataFrame(df_oneHot_standard, columns=df_oneHot_encoded.iloc[:,0:13].columns)\n",
    "df_oneHot_robust = pd.DataFrame(df_oneHot_robust, columns=df_oneHot_encoded.iloc[:,0:13].columns)\n",
    "df_oneHot_minmax = pd.DataFrame(df_oneHot_minmax, columns=df_oneHot_encoded.iloc[:,0:13].columns)\n",
    "df_oneHot_maxabs = pd.DataFrame(df_oneHot_maxabs, columns=df_oneHot_encoded.iloc[:,0:13].columns)\n",
    "df_oneHot_normalizer = pd.DataFrame(df_oneHot_normalizer, columns=df_oneHot_encoded.iloc[:,0:13].columns)\n",
    "\n",
    "data_list = ['df_label_standard', 'df_label_robust', 'df_label_minmax', 'df_label_maxabs', \n",
    "\n",
    "             'df_oneHot_standard', 'df_oneHot_robust','df_oneHot_minmax' , 'df_oneHot_maxabs', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a618fbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# K-means\n",
    "for K in [4, 5, 6] :\n",
    "    print(\"\\n__Result of K-Means Clustering, K : \", K)\n",
    "\n",
    "    for data in data_list:\n",
    "        clust_model = KMeans(n_clusters = K, random_state = 42, algorithm = 'auto')\n",
    "        clust_model.fit(eval(data))\n",
    "        centers = clust_model.cluster_centers_\n",
    "        pred = clust_model.predict(eval(data))\n",
    "        df_clusted = eval(data).copy()\n",
    "        df_clusted['cluster'] = pred\n",
    "\n",
    "        #Silhouette score\n",
    "        score = silhouette_score(eval(data), pred, metric=\"euclidean\")\n",
    "        print(data, \": Silhouette Score:\", score)\n",
    "\n",
    "# Visualization\n",
    "fig = plt.figure(figsize = (8, 8))\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "X = df_clusted\n",
    "\n",
    "ax.scatter(X.iloc[:,0],X.iloc[:,1],X.iloc[:,2], c = X.cluster, s = 10, cmap = 'rainbow', alpha = 1)\n",
    "ax.scatter(centers[:,0],centers[:,1],centers[:,2], c = 'black', marker = '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ce17a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GMM\n",
    "for K in [4, 5, 6] :\n",
    "    print(\"\\n__Result of GMM Clustering, K : \", K)\n",
    "    for data in data_list:\n",
    "        gmm = GaussianMixture(n_components = K, random_state = 42)\n",
    "        gmm.fit(eval(data))\n",
    "        gmm_cluster_labels = gmm.predict(eval(data))\n",
    "        df_clusted = eval(data).copy()\n",
    "        df_clusted['gmm_cluster'] = gmm_cluster_labels\n",
    "\n",
    "        #Silhouette score\n",
    "        score = silhouette_score(eval(data), gmm_cluster_labels, metric=\"euclidean\")\n",
    "        print(data, \"Silhouette Score:\", score)\n",
    "    \n",
    "\n",
    "\n",
    "# Visualization\n",
    "fig = plt.figure(figsize = (8, 8))\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "X = df_clusted\n",
    "\n",
    "ax.scatter(X.iloc[:,0],X.iloc[:,1],X.iloc[:,2], c = X.gmm_cluster, s = 10, cmap = 'rainbow', alpha = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4324e6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CLARANS    \n",
    "\n",
    "\"\"\"\n",
    "The pyclustering library clarans implementation requires\n",
    "list of lists as its input dataset.\n",
    "Thus we convert the data from numpy array to list.\n",
    "\"\"\"\n",
    "df_label_standard = df_label_standard.values.tolist()\n",
    "\n",
    "K = 3\n",
    "I = 100\n",
    "N = 10\n",
    "\n",
    "clarans_instance = clarans(df_label_standard, K, I, N)\n",
    "\n",
    "clarans_instance.process()\n",
    "\n",
    "clusters = clarans_instance.get_clusters()\n",
    "medoids = clarans_instance.get_medoids()\n",
    "\n",
    "print(\"Index of the points that are in a cluster : \", clusters)\n",
    "print(\"The index of medoids that algorithm found to be best : \", medoids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ecd4c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "\n",
    "for e in [0.4, 0.6] :\n",
    "    for threshold in [10, 15] :\n",
    "        print(\"e : \", e, \" threshold : \" , threshold) \n",
    "        for data in data_list:\n",
    "            model = DBSCAN(eps=e, min_samples=threshold)\n",
    "            model.fit(eval(data))\n",
    "            df_scaled = eval(data).copy()\n",
    "            df_scaled['cluster'] = model.fit_predict(eval(data))\n",
    "\n",
    "            #Silhouette score\n",
    "            score = silhouette_score(df_scaled, df_scaled['cluster'], metric=\"euclidean\")\n",
    "            print(data, \"Silhouette Score:\", score)\n",
    "\n",
    "print(\"\\n__Result of DBSCAN Clustering **Outlier = -1**\\n\")\n",
    "\n",
    "# Visualization\n",
    "fig = plt.figure(figsize = (8, 8))\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "X = df_scaled\n",
    "\n",
    "ax.scatter(X.iloc[:,0],X.iloc[:,1],X.iloc[:,2], c = X.cluster, s = 10, cmap = 'rainbow', alpha = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc54953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K = 4\n",
    "\n",
    "for data in data_list:\n",
    "    sc = SpectralClustering(n_clusters = K).fit(eval(data))\n",
    "    print(data, sc.labels_)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb756b18",
   "metadata": {},
   "source": [
    "> ### Choose Parmeters :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec66523",
   "metadata": {},
   "source": [
    "# 3. Function \"AutoML\"\n",
    "> ### A function AutoML is a all-in-one classifier function. It Automatically runs different combinations of the below : \n",
    "> -\tData Scaling, Data Encoding\n",
    "> -\tVarious Clustering Algorithms\n",
    "> -\tVarious quality measuring tools\n",
    "> -\tVarious values of parameters and hyperparameters\n",
    "> -\tVarious subsets of the features of the dataset\n",
    "***\n",
    "> ### AutoML (scaler_list, encoder_list, model_list, training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c91ef19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def autoML(encoder_list, scaler_list, model_list, df):\n",
    "\n",
    "    \n",
    "    # Set supported scalers, encofders and models\n",
    "    encoder = ['LabelEncoder()', 'OneHotEncoder()']\n",
    "    scaler = ['StandardScaler()', 'MinMaxScaler()', 'RobustScaler()', 'MaxAbsScaler()', 'Normalizer()']\n",
    "    model = ['KMeans', 'GMM', 'CLARANS', 'DBSCAN', 'Spectral']\n",
    "    \n",
    "    # Verify that the autoML function supports the scaler, encoder, and model entered as a function parameter.\n",
    "    notspt = False;\n",
    "    for e in encoder_list:\n",
    "        if e not in encoder:\n",
    "            print(\"The entered encoder\", e, \"is not supported in the autoML function.\\n\")\n",
    "            notspt = True;\n",
    "    for s in scaler_list:\n",
    "        if s not in scaler:\n",
    "            print(\"The entered scaler\", s, \"is not supported in the autoML function.\\n\")\n",
    "            notspt = True;\n",
    "    for m in model_list:\n",
    "        if m not in model:\n",
    "            print(\"The entered model\", m, \"is not supported in the autoML function.\\n\")\n",
    "            notspt = True;\n",
    "    if notspt:\n",
    "        print(\"Try again with supported parameters.\\n\")\n",
    "        return\n",
    "    \n",
    "    for e in encoder_list:\n",
    "        \n",
    "        # Encoding (attribute 'ocean_proximity')\n",
    "        if 'ocean_proximity' in df.columns:\n",
    "            label = df['ocean_proximity']\n",
    "            df_encoded = df.drop(labels= 'ocean_proximity', axis = 1)\n",
    "            if e == 'LabelEncoder()':\n",
    "                le = LabelEncoder()\n",
    "                le.fit(label)\n",
    "                label_encoded = le.transform(label)\n",
    "                df_label_encoded = pd.DataFrame(label_encoded, columns = ['labelEncoded_oceanProximity'])\n",
    "                df_encoded = pd.concat([df_encoded, df_label_encoded], axis = 1)\n",
    "                print(\"__\", e, \" Finished.\")\n",
    "\n",
    "            if e == 'OneHotEncoder()':\n",
    "                df_oneHot_encoded = pd.get_dummies(label)\n",
    "                df_encoded = pd.concat([df_encoded, df_oneHot_encoded], axis = 1)\n",
    "                print(\"__\", e, \" Finished.\")\n",
    "        else :\n",
    "            df_encoded = df\n",
    "        # Scale\n",
    "        for s in scaler_list:\n",
    "            # Scale\n",
    "            if s == 'StandardScaler()':\n",
    "                df_scaled = StandardScaler().fit_transform(df_encoded)\n",
    "            if s == 'RobustScaler()':\n",
    "                df_sclaed = RobustScaler().fit_transform(df_encoded)\n",
    "            if s == 'MinMaxScaler()':\n",
    "                df_scaled = MinMaxScaler().fit_transform(df_encoded)\n",
    "            if s == 'MaxAbsScaler()':\n",
    "                df_scaled = MaxAbsScaler().fit_transform(df_encoded)\n",
    "            if s == 'Normalizer()':\n",
    "                df_scaled = Normalizer().fit_transform(df_encoded)\n",
    "            # Change format of scaled data (np.arrays -> DataFrame)\n",
    "            if e == 'LabelEncoder()':\n",
    "                cols = df_scaled.shape[1]\n",
    "                df_scaled = pd.DataFrame(df_scaled, columns=df_encoded.iloc[:,0:cols].columns)\n",
    "            if e == 'OneHotEncoder()': \n",
    "                cols = df_scaled.shape[1]\n",
    "                df_scaled = pd.DataFrame(df_scaled, columns=df_encoded.iloc[:,0:cols].columns)\n",
    "            \n",
    "            for m in model_list:\n",
    "                # Clustering\n",
    "                # K-Means\n",
    "                if m == 'KMeans':\n",
    "                    K = 6\n",
    "                    clust_model = KMeans(n_clusters = K, random_state = 42, algorithm = 'auto')\n",
    "                    clust_model.fit(df_scaled)\n",
    "\n",
    "                    centers = clust_model.cluster_centers_\n",
    "                    pred = clust_model.predict(df_scaled)\n",
    "\n",
    "                    df_clusted = df_scaled.copy()\n",
    "                    df_clusted['cluster'] = pred\n",
    "\n",
    "                    print(\"\\n__Result of K-Means Clustering\\n\")\n",
    "                    print(\"Scaler : \",s,\"Encoder : \", e)\n",
    "                    print(\"Column list : \", df.columns.to_list())\n",
    "\n",
    "                    #Silhouette score\n",
    "                    score = silhouette_score(df_scaled, pred, metric=\"euclidean\")\n",
    "                    print(\"Silhouette Score:\", score)\n",
    "\n",
    "                    # Visualization\n",
    "                    fig = plt.figure(figsize = (8, 8))\n",
    "                    ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "                    X = df_clusted\n",
    "\n",
    "                    ax.scatter(X.iloc[:,0],X.iloc[:,1],X.iloc[:,2], c = X.cluster, s = 10, cmap = 'rainbow', alpha = 1)\n",
    "                    ax.scatter(centers[:,0],centers[:,1],centers[:,2], c = 'black', marker = '*')\n",
    "                    plt.show()\n",
    "\n",
    "                # GMM\n",
    "                if m == 'GMM':\n",
    "                    K = 6\n",
    "                    gmm = GaussianMixture(n_components = K, random_state = 42)\n",
    "                    gmm.fit(df_scaled)\n",
    "                    gmm_cluster_labels = gmm.predict(df_scaled)\n",
    "\n",
    "                    df_clusted = df_scaled.copy()\n",
    "                    df_clusted['gmm_cluster'] = gmm_cluster_labels\n",
    "\n",
    "                    print(\"\\n__Result of GMM Clustering\\n\")\n",
    "                    print(\"Scaler : \",s,\"Encoder : \", e)\n",
    "                    print(\"Column list : \", df.columns.to_list())\n",
    "                    \n",
    "                    #Silhouette score\n",
    "                    score = silhouette_score(df_scaled, gmm_cluster_labels, metric=\"euclidean\")\n",
    "                    print(\"Silhouette Score:\", score)\n",
    "\n",
    "                    # Visualization\n",
    "                    fig = plt.figure(figsize = (8, 8))\n",
    "                    ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "                    X = df_clusted\n",
    "\n",
    "                    ax.scatter(X.iloc[:,0],X.iloc[:,1],X.iloc[:,2], c = X.gmm_cluster, s = 10, cmap = 'rainbow', alpha = 1)\n",
    "                    plt.show()\n",
    "\n",
    "                # CLARANS    \n",
    "                if m == 'CLARANS':\n",
    "                    \"\"\"\n",
    "                    The pyclustering library clarans implementation requires\n",
    "                    list of lists as its input dataset.\n",
    "                    Thus we convert the data from numpy array to list.\n",
    "                    \"\"\"\n",
    "                    df_scaled = df_scaled.values.tolist()\n",
    "\n",
    "                    K = 4\n",
    "                    I = int(input(\"Input the amount of iterations: \"))\n",
    "                    N = int(input(\"Input the number of max neighbors: \"))\n",
    "\n",
    "                    clarans_instance = clarans(df_scaled, K, I, N)\n",
    "\n",
    "                    clarans_instance.process()\n",
    "\n",
    "                    clusters = clarans_instance.get_clusters()\n",
    "                    medoids = clarans_instance.get_medoids()\n",
    "\n",
    "                    print(\"Index of the points that are in a cluster : \", clusters)\n",
    "                    print(\"The index of medoids that algorithm found to be best : \", medoids)\n",
    "\n",
    "                # DBSCAN\n",
    "                if m == 'DBSCAN':\n",
    "                    eps = 0.6\n",
    "                    threshold = 10\n",
    "\n",
    "                    model = DBSCAN(eps=eps, min_samples=threshold)\n",
    "                    model.fit(df_scaled)\n",
    "                    df_scaled['cluster'] = model.fit_predict(df_scaled)\n",
    "\n",
    "                    print(\"\\n__Result of DBSCAN Clustering **Outlier = -1**\\n\")\n",
    "                    print(\"Scaler : \",s,\"Encoder : \", e)\n",
    "                    print(\"Column list : \", df.columns.to_list())\n",
    "                    \n",
    "                    #Silhouette score\n",
    "                    score = silhouette_score(df_scaled, df_scaled['cluster'], metric=\"euclidean\")\n",
    "                    print(\"Silhouette Score:\", score)\n",
    "\n",
    "                    # Visualization\n",
    "                    fig = plt.figure(figsize = (8, 8))\n",
    "                    ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "                    X = df_scaled\n",
    "\n",
    "                    ax.scatter(X.iloc[:,0],X.iloc[:,1],X.iloc[:,2], c = X.cluster, s = 10, cmap = 'rainbow', alpha = 1)\n",
    "                    plt.show()\n",
    "\n",
    "                if m == 'Spectral':\n",
    "                    K = int(input(\"Input the number of clusters: \"))\n",
    "\n",
    "                    sc = SpectralClustering(n_clusters = K).fit(df_scaled)\n",
    "                    print(sc.labels_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191639e9",
   "metadata": {},
   "source": [
    "> ## Parameters::\n",
    ">> ### scaler_list, default=None\n",
    ">>  if None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1.\n",
    ">>  <br>**Supported Scaler = ['StandardScaler()', 'MinMaxScaler()', 'RobustScaler()', 'MaxAbsScaler()', 'Normalizer()']**\n",
    ">>  <br> Only supported arguments are available. Else, error returned\n",
    ">> ### encoder_list, default=None\n",
    ">>  if None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1.\n",
    "    <br>**Supported Encoder = ['LabelEncoder()', 'OneHotEncoder()']**\n",
    "    >>  <br> Only supported arguments are available. Else, error returned\n",
    ">> ### model_list, default=None\n",
    ">>  if None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1.\n",
    "    <br>**Supported Model = ['KMeans', 'GMM', 'CLARANS', 'DBSCAN', 'Spectral']**\n",
    "    >>  <br> Only supported arguments are available. Else, error returned\n",
    ">> ### training_dataset, default=None\n",
    ">>  if None, then the function returns error\n",
    "> ## Attributes::\n",
    ">> ### encoder\n",
    ">>  encoder = ['LabelEncoder()', 'OneHotEncoder()']\n",
    ">> ### scaler\n",
    ">> scaler = ['StandardScaler()', 'MinMaxScaler()', 'RobustScaler()', 'MaxAbsScaler()', 'Normalizer()']\n",
    ">> ### model\n",
    ">> model = ['KMeans', 'GMM', 'CLARANS', 'DBSCAN', 'Spectral']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7708a",
   "metadata": {},
   "source": [
    "# 4. Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888319f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler, Normalizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from pyclustering.cluster.clarans import clarans\n",
    "from pyclustering.utils import timedcall\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "def load_df(attList):\n",
    "    # Data exploration\n",
    "    df = pd.read_csv('housing.csv', sep=',')\n",
    "    print(\"__California Housing Prices Dataset\\n\")\n",
    "    # Drop the target value: \"median_house_value\"\n",
    "    df.drop(df.columns[8], axis = 1, inplace = True)\n",
    "    print(\"__Drop attribute 'median_house_value', because it is the answer value\\n\")\n",
    "    # replace NA to MEDIAN\n",
    "    df = df.replace(np.nan,int(df.iloc[:, 5].median()))\n",
    "    print(\"__NA replaced to MEDIAN value.\\n\")\n",
    "    \n",
    "    df2 = pd.DataFrame()\n",
    "    for i in attList:\n",
    "        if(i in df.columns):\n",
    "            df2[i] = df[i]\n",
    "            \n",
    "    return df2\n",
    "\n",
    "\n",
    "####이렇게 함수 활용해야 함\n",
    "encoder_list_1 = ['LabelEncoder()', 'OneHotEncoder()']\n",
    "scaler_list_1 = ['StandardScaler()', 'MinMaxScaler()', 'RobustScaler()', 'MaxAbsScaler()', 'Normalizer()']\n",
    "model_list_1 = ['KMeans', 'GMM', 'DBSCAN', 'Spectral']\n",
    "\n",
    "encoder_list_2 = ['LabelEncoder()', 'OneHotEncoder()']\n",
    "scaler_list_2 = ['StandardScaler()', 'MinMaxScaler()', 'RobustScaler()']\n",
    "model_list_2 = ['KMeans', 'GMM', 'DBSCAN']\n",
    "\n",
    "att_list1 = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income',\n",
    "            'median_house_value','ocean_proximity']\n",
    "\n",
    "att_list2 = ['longitude','latitude','housing_median_age','total_rooms']\n",
    "\n",
    "dataset_1 = load_df(att_list1)\n",
    "\n",
    "dataset_2 = load_df(att_list2)\n",
    "\n",
    "# autoML(encoder_list_1, scaler_list_1, model_list_1, dataset_1)\n",
    "# autoML(encoder_list_1, scaler_list_1, model_list_1, dataset_2)\n",
    "\n",
    "autoML(encoder_list_2, scaler_list_2, model_list_2, dataset_1)\n",
    "autoML(encoder_list_2, scaler_list_2, model_list_2, dataset_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d70fc",
   "metadata": {},
   "source": [
    "# 5. Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d36b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from pyclustering.cluster.clarans import clarans\n",
    "from pyclustering.utils import timedcall\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import *\n",
    "from pyclustering.cluster import cluster_visualizer_multidim\n",
    "\n",
    "def load_df(attList):\n",
    "    # Data exploration\n",
    "    df = pd.read_csv('housing.csv', sep=',')\n",
    "    print(\"__California Housing Prices Dataset\\n\")\n",
    "    # Drop the target value: \"median_house_value\"\n",
    "    df.drop(df.columns[8], axis=1, inplace=True)\n",
    "    print(\"__Drop attribute 'median_house_value', because it is the answer value\\n\")\n",
    "    # replace NA to MEDIAN\n",
    "    df = df.replace(np.nan, int(df.iloc[:, 5].median()))\n",
    "    print(\"__NA replaced to MEDIAN value.\\n\")\n",
    "\n",
    "    df2 = pd.DataFrame()\n",
    "    for i in attList:\n",
    "        if (i in df.columns):\n",
    "            df2[i] = df[i]\n",
    "\n",
    "    return df2\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # Compute confusion matrix\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # Return purity score\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "encoder_list = ['LabelEncoder()', 'OneHotEncoder()']\n",
    "\n",
    "scaler_list_1 = ['StandardScaler()', 'MinMaxScaler()', 'RobustScaler()', 'MaxAbsScaler()']\n",
    "model_list_1 = ['KMeans', 'GMM','DBSCAN']\n",
    "\n",
    "scaler_list_2 = ['StandardScaler()', 'MinMaxScaler()', 'RobustScaler()', 'MaxAbsScaler()']\n",
    "model_list_2 = ['CLARANS', 'MeanShift']\n",
    "\n",
    "att_list1 = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households',\n",
    "             'median_income', 'median_house_value', 'ocean_proximity']\n",
    "att_list2 = ['longitude', 'latitude', 'housing_median_age', 'total_rooms']\n",
    "\n",
    "dataset_1 = load_df(att_list1)\n",
    "dataset_2 = load_df(att_list2)\n",
    "\n",
    "df = pd.read_csv('housing.csv', sep=',')\n",
    "\n",
    "# Divide target value in 4 quantiles\n",
    "quantiles = list(df['median_house_value'].quantile([0.25, 0.5, 0.75, 1.0]))\n",
    "df.loc[df['median_house_value'] >= quantiles[0], 'quantiles'] = 1\n",
    "df.loc[df['median_house_value'] >= quantiles[1], 'quantiles'] = 2\n",
    "df.loc[df['median_house_value'] >= quantiles[2], 'quantiles'] = 3\n",
    "df.loc[df['median_house_value'] >= quantiles[3], 'quantiles'] = 4\n",
    "\n",
    "y = df['quantiles'].astype(\"category\")\n",
    "\n",
    "autoML(encoder_list, scaler_list_1, model_list_1, dataset_1, y)\n",
    "autoML(encoder_list, scaler_list_2, model_list_2, dataset_2, y)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
